{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "029cc9fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install XGBoost\n",
    "#!pip install git+https://github.com/smazzanti/mrmr\n",
    "    \n",
    "# Common use \n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "\n",
    "#Metrics\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from rdkit import Chem\n",
    "\n",
    "#Feature selection\n",
    "from mrmr import mrmr_classif\n",
    "\n",
    "#Models - Regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "#Models - Classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca5f7e2",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c705c104",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_overview(df, split):\n",
    "    print('Total number of molecules:',len(df))\n",
    "    print('Train set: %d - %0.1f%%' %(len(split['train']), len(split['train'])/len(df)*100))\n",
    "    print('Validation set: %d - %0.1f%%' %(len(split['valid']), len(split['valid'])/len(df)*100))\n",
    "    print('Test set: %d - %0.1f%%' %(len(split['test']), len(split['test'])/len(df)*100))\n",
    "    display(split['train'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "00ba334f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute regression metrics \n",
    "def get_metrics(model_name, y_true, y_pred, c, mode = 0):\n",
    "    \n",
    "    # Compute metrics \n",
    "    if c == False:\n",
    "        var1 = metrics.r2_score(y_true, y_pred)\n",
    "        var2 = metrics.mean_absolute_error(y_true, y_pred)\n",
    "        var3 = metrics.mean_squared_error(y_true, y_pred)\n",
    "        \n",
    "        print(model_name,'| R2: %0.3f, MAE: %0.3f, MSE: %0.3f' %(var1, var2, var3))\n",
    "        \n",
    "        # Mode to 1 displays r-squared plots\n",
    "        if mode == 1: \n",
    "            plt.figure(figsize=(10, 5), dpi = 95)\n",
    "\n",
    "            plt.scatter(y_true, y_pred, color='salmon', s=5)\n",
    "            plt.plot(np.unique(y_true), np.poly1d(np.polyfit(y_true, y_pred, 1))(np.unique(y_true)), color='black')\n",
    "\n",
    "            plt.text(0, 3.5,'R-squared = %0.2f' % var1)\n",
    "            plt.xlabel('Actual values')\n",
    "            plt.ylabel('Predicted Values')\n",
    "            plt.title('Prediction results using {}'.format(model_name))\n",
    "            plt.show()\n",
    "        \n",
    "        return var1, var2, var3\n",
    "        \n",
    "    else:\n",
    "        var1 = metrics.matthews_corrcoef(y_true, y_pred)\n",
    "        var2 = metrics.roc_auc_score(y_true, y_pred)\n",
    "        var3 = metrics.precision_score(y_true, y_pred)\n",
    "        var4 = metrics.recall_score(y_true, y_pred)\n",
    "        var5 = metrics.accuracy_score(y_true, y_pred)\n",
    "        \n",
    "        print(model_name,'| MCC: %0.3f, AUC: %0.3f, Precision: %0.3f, Recall: %0.3f, Accuracy: %0.3f' \n",
    "              %(var1, var2, var3, var4, var5))\n",
    "        \n",
    "    return var1, var2, var3, var4, var5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27eddc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comparison(names, met, c):\n",
    "    sns.set(font_scale = 1)\n",
    "    # set width of bar\n",
    "    barWidth = 0.15\n",
    "    plt.figure(figsize=(15, 6), dpi = 95)\n",
    "\n",
    "    # set height of bar\n",
    "    var1 = [i[0] for i in met]\n",
    "    var2 = [i[1] for i in met]\n",
    "    var3 = [i[2] for i in met]\n",
    "    if c == True:\n",
    "        var4 = [i[3] for i in met]\n",
    "        var5 = [i[4] for i in met]\n",
    "\n",
    "    # Set position of bar on X axis\n",
    "    br1 = np.arange(len(var1))\n",
    "    br2 = [x + barWidth for x in br1]\n",
    "    br3 = [x + barWidth for x in br2]\n",
    "    br4 = [x + barWidth for x in br3]\n",
    "    br5 = [x + barWidth for x in br4]\n",
    "\n",
    "    # Make the plot\n",
    "    if c == False:\n",
    "        plt.bar(br1, var1, color ='r', width = barWidth,\n",
    "                edgecolor ='grey', label ='R2')\n",
    "        plt.bar(br2, var2, color ='g', width = barWidth,\n",
    "                edgecolor ='grey', label ='MAE')\n",
    "        plt.bar(br3, var3, color ='b', width = barWidth,\n",
    "                edgecolor ='grey', label ='MSE')\n",
    "    \n",
    "    # Add 2 more bars \n",
    "    else :\n",
    "        plt.bar(br1, var1, color ='r', width = barWidth,\n",
    "                edgecolor ='grey', label ='MCC')\n",
    "        plt.bar(br2, var2, color ='g', width = barWidth,\n",
    "                edgecolor ='grey', label ='AUC')\n",
    "        plt.bar(br3, var3, color ='b', width = barWidth,\n",
    "                edgecolor ='grey', label ='ACC')\n",
    "        plt.bar(br4, var4, color ='yellow', width = barWidth,\n",
    "            edgecolor ='grey', label ='PRE')\n",
    "        plt.bar(br5, var5, color ='purple', width = barWidth,\n",
    "            edgecolor ='grey', label ='REC')\n",
    "    \n",
    "    # Adding Xticks\n",
    "    plt.xlabel('Models', fontweight ='bold', fontsize = 15)\n",
    "    plt.ylabel('Metrics', fontweight ='bold', fontsize = 15)\n",
    "    plt.xticks([r + barWidth for r in range(len(var1))],\n",
    "            [name for name in names])\n",
    "    plt.legend()\n",
    "    plt.title('Metrics obtained for each model')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a5dfc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We visualize the distributions of the columns that contain NaN values in order to decide which value should be replaced with\n",
    "\n",
    "# import matplotlib.pyplot as plt \n",
    "\n",
    "# fig, axs = plt.subplots(4, 3, figsize=(15,15))\n",
    "# fig.tight_layout()\n",
    "\n",
    "# def iterate_columns(cols, counter):\n",
    "#     for ind, col in enumerate(cols):\n",
    "#         col.hist(df[nan_cols[ind+counter]])\n",
    "#         col.axvline(df[nan_cols[ind+counter]].mean(), color='k', linestyle='dashed', linewidth=1, label='Mean')\n",
    "#         col.axvline(df[nan_cols[ind+counter]].median(), color='r', linestyle='dashed', linewidth=1, label='Median')\n",
    "#         col.legend()\n",
    "#         col.set_title(nan_cols[ind+counter])\n",
    "\n",
    "# counter = [0,3,6,9]\n",
    "# aux = 0\n",
    "# for row in axs:\n",
    "#     iterate_columns(row, counter[aux])\n",
    "#     aux += 1\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc71d1be",
   "metadata": {},
   "source": [
    "## Functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "805000b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model \n",
    "# joblib.dump(rf_lipo_baseline, 'rf_lipo_baseline.pkl')\n",
    "\n",
    "# Loading model\n",
    "# rf_model = joblib.load('rf_lipo_baseline.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dad29a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_ha_duplicates(df):\n",
    "    og_shape = df.shape\n",
    "    heavy_atoms = pd.Series([Chem.MolFromSmiles(smi).GetNumHeavyAtoms() for smi in df['Drug']])\n",
    "    # DF now only contains compounds with 5 or more heavy atoms\n",
    "    df = df[heavy_atoms >= 5]\n",
    "    # Duplicates removal from DF\n",
    "    df.drop(df[df['Drug'].duplicated()].index, inplace = True)\n",
    "    \n",
    "    print('Duplicated compounds and with less than 5 heavy atoms have been removed.')\n",
    "    print('New number of compounds: %d (%d)' %(df.shape[0], (df.shape[0] - og_shape[0])))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "387fbd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_benchmark(benchmark, models, results, manual = False):\n",
    "    temp = [r2[0] for r2 in results]\n",
    "    # (name of model, r2 score, ID). ID is previousID +1\n",
    "    if len(benchmark) == 0:\n",
    "        benchmark.append((models[np.argmax(temp)], np.max(temp), 0))\n",
    "    else:\n",
    "        benchmark.append((models[np.argmax(temp)], np.max(temp), benchmark[len(benchmark)-1][2]+1))\n",
    "    \n",
    "    \n",
    "    return benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4037478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove molecules that contain many zero-value/nan features\n",
    "def remove_nans(X_train, X_val, X_test, y_train, y_val, y_test, X_train_fps, X_val_fps):\n",
    "    for ind, dset in enumerate([X_train, X_val, X_test]):\n",
    "        nan_mols = dset.isnull().any(axis=1)\n",
    "        if len(dset[nan_mols].index) !=0 :\n",
    "            # Check rows where NaN values are located \n",
    "            nan_rows = dset[nan_mols].index\n",
    "            # Remove NaNs in dataset\n",
    "            dset.drop(index = nan_rows, inplace = True)\n",
    "            if ind == 0:\n",
    "                y_train.drop(index = nan_rows, inplace = True)\n",
    "                X_train_fps.drop(index = nan_rows, inplace = True)\n",
    "                print('Removed the following rows in the train set:',nan_rows)\n",
    "\n",
    "            elif ind == 1:\n",
    "                X_val_fps.drop(index = nan_rows, inplace = True)\n",
    "                y_val.drop(index = nan_rows, inplace = True)\n",
    "                print('Removed the following rows in the val set:',nan_rows)\n",
    "\n",
    "            else:\n",
    "                y_test.drop(index = nan_rows, inplace = True)\n",
    "                print('Removed the following rows in the test set:',nan_rows)\n",
    "        \n",
    "    return X_train, X_train_fps, y_train, X_val, X_val_fps, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64586be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_split(df, split, fps):\n",
    "    \n",
    "    # Training\n",
    "    X_train = df[df.columns[3:-1]][:len(split['train'])]\n",
    "    y_train = df['Y'][:len(split['train'])]\n",
    "    #Same with fingerprints (Since Y is the same, we just need to care of X)\n",
    "    X_train_fps = fps[fps.columns[:]][:len(split['train'])]\n",
    "    \n",
    "    \n",
    "    # Validation\n",
    "    X_val = df[df.columns[3:-1]][len(split['train']):(len(split['valid'])+len(split['train']))]\n",
    "    y_val = df['Y'][len(split['train']):(len(split['valid'])+len(split['train']))]\n",
    "    #Same with fingerprints (Since Y is the same, we just need to care of X)\n",
    "    X_val_fps = fps[fps.columns[:]][len(split['train']):(len(split['valid'])+len(split['train']))]\n",
    "\n",
    "    \n",
    "    # Test\n",
    "    X_test = df[df.columns[3:-1]][(len(split['valid'])+len(split['train'])):len(df)]\n",
    "    y_test = df['Y'][(len(split['valid'])+len(split['train'])):len(df)]\n",
    "    #Same with fingerprints (Since Y is the same, we just need to care of X)\n",
    "    X_test_fps = fps[fps.columns[:]][(len(split['valid'])+len(split['train'])):len(df)]\n",
    "    \n",
    "    print('Data has been split')\n",
    "    \n",
    "    return X_train, X_train_fps, y_train, X_val, X_val_fps, y_val, X_test, X_test_fps, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5077d6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(X_train, X_val, X_test):\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # convert in into DF after standardizing \n",
    "    X_train_norm = scaler.fit_transform(X_train)\n",
    "    X_train_norm = pd.DataFrame(X_train_norm, columns = X_train.columns)\n",
    "    \n",
    "    X_val_norm = scaler.transform(X_val)\n",
    "    X_val_norm = pd.DataFrame(X_val_norm, columns = X_val.columns)\n",
    "    \n",
    "    X_test_norm = scaler.transform(X_test)\n",
    "    X_test_norm = pd.DataFrame(X_test_norm, columns = X_test.columns)\n",
    "    \n",
    "    print('Data is now normalized.')\n",
    "    return X_train_norm, X_val_norm, X_test_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a54b334",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fs_mrmr(X_train, y_train, X_val, y_val, X_train_norm, X_val_norm, c = False):\n",
    "    num_sel_feat = np.arange(10, X_train.shape[1], 5)\n",
    "    m,r,f = [], [], []\n",
    "    for k in num_sel_feat:\n",
    "        selected_features = mrmr_classif(X_train, y_train, K = k)\n",
    "        f.append(selected_features)\n",
    "        # Get metrics based on selected features\n",
    "        models, results = models_comparison(X_train[selected_features], y_train, X_val[selected_features], y_val,\n",
    "                                        c, False, False, X_train_norm[selected_features], X_val_norm[selected_features])\n",
    "        m.append(models)\n",
    "        r.append(results)\n",
    "    \n",
    "    return m,r,num_sel_feat,f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "554a4122",
   "metadata": {},
   "outputs": [],
   "source": [
    "def models_comparison(X_train, y_train, X_val, y_val, c, plot = True, fps = False \n",
    "                          , X_train_norm = 0, X_val_norm = 0):\n",
    "    np.random.seed(10)\n",
    "    n_models = ['LR1', 'RFR', 'DTR', 'SVR', 'MLPR', 'XGBR', 'MLPC', 'SVC', 'RFC', 'DTC', 'XGBC']\n",
    "    models = {'Regression': \n",
    "              {'Linear': {'LR': LinearRegression(), 'SVR': SVR(), \n",
    "                          'MLPR': MLPRegressor(hidden_layer_sizes=(128,64,32), max_iter=500)},\n",
    "               'Non-linear':{'RFR': RandomForestRegressor(), 'DTR': DecisionTreeRegressor(),  \n",
    "                             'XGBR':XGBRegressor()}},\n",
    "                  \n",
    "              'Classification': \n",
    "              {'Linear': {'SVC': SVC(), \n",
    "                          'MLPC': MLPClassifier(hidden_layer_sizes=(128,64,32), max_iter=500)},\n",
    "               'Non-linear':{'RFC': RandomForestClassifier(), 'DTC': DecisionTreeClassifier(),\n",
    "                            'XGBC':XGBClassifier()}\n",
    "              }\n",
    "            }\n",
    "\n",
    "    names = []\n",
    "    results = []\n",
    "    \n",
    "    for name in n_models:\n",
    "        #REGRESSION\n",
    "        if name in models['Regression']['Linear'].keys() and c == False:\n",
    "            clf = models['Regression']['Linear'][name].fit(X_train_norm, y_train)\n",
    "            y_pred = clf.predict(X_val_norm)\n",
    "            \n",
    "            names.append(name)\n",
    "            r2, mae, mse = get_metrics(name, y_val, y_pred, c)\n",
    "            results.append([r2, mae, mse])\n",
    "            \n",
    "        elif name in models['Regression']['Non-linear'].keys() and c == False:\n",
    "            clf = models['Regression']['Non-linear'][name].fit(X_train, y_train)\n",
    "            y_pred = clf.predict(X_val)\n",
    "            \n",
    "            names.append(name)\n",
    "            r2, mae, mse = get_metrics(name, y_val, y_pred, c)\n",
    "            results.append([r2, mae, mse])\n",
    "        \n",
    "        #CLASSIFICATION\n",
    "        elif name in models['Classification']['Linear'].keys() and c == True:\n",
    "            clf = models['Classification']['Linear'][name].fit(X_train_norm, y_train)\n",
    "            y_pred = clf.predict(X_val_norm)\n",
    "            \n",
    "            names.append(name)\n",
    "            mcc, auc, acc, pre, rec = get_metrics(name, y_val, y_pred, c)\n",
    "            results.append([mcc, auc, acc, pre, rec])\n",
    "            \n",
    "        elif name in models['Classification']['Non-linear'].keys() and c == True :\n",
    "            clf = models['Classification']['Non-linear'][name].fit(X_train, y_train)    \n",
    "            y_pred = clf.predict(X_val)\n",
    "\n",
    "            names.append(name)\n",
    "            mcc, auc, acc, pre, rec = get_metrics(name, y_val, y_pred, c)\n",
    "            results.append([mcc, auc, acc, pre, rec])\n",
    "        \n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    # We plot the metrics obtained for each model\n",
    "    if plot == True:\n",
    "        plot_comparison(names, results, c)\n",
    "    return names, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477e2c22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5ae2584",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fs_relieff_selected_features(X_train, y_train, X_val, y_val, X_train_norm, X_val_norm, c, n_neighbors):\n",
    "    \n",
    "    #num_sel_feat = np.arange(10, X_train.shape[1], 5)\n",
    "    \n",
    "    # Use selected feature sets only\n",
    "    num_sel_feat = np.arange(30, X_train.shape[1], 30)\n",
    "        \n",
    "    m,r,f = [], [], []\n",
    "    \n",
    "    for k in num_sel_feat:\n",
    "        print(\"\\n=======================Selected features %0.0f/%0.0f =======================\"%(k,X_train.shape[1]))\n",
    "        # Applying relieff feature selection on train, validation and normalized datasets\n",
    "        fs = ReliefF(n_features_to_select=k, n_neighbors=n_neighbors)\n",
    "        \n",
    "        fs.fit_transform(X_train.to_numpy(), y_train.to_numpy())\n",
    "        pos = pd.DataFrame(fs.feature_importances_.reshape(-1,1)).sort_values(by=0, ascending=False).head(k).index.tolist()\n",
    "        selected_features = list(X_train.columns[pos])\n",
    "\n",
    "        f.append(selected_features) \n",
    "        \n",
    "        # Get metrics based on selected features\n",
    "        models, results = models_comparison(X_train[selected_features], y_train, X_val[selected_features], y_val,\n",
    "                            c, False, False, X_train_norm[selected_features], X_val_norm[selected_features])\n",
    "                \n",
    "        m.append(models)\n",
    "        r.append(results)\n",
    "        \n",
    "    return m,r,num_sel_feat,f "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0bfd1526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADDED\n",
    "# Applying various feature selection algorithm supplied by the score_func\n",
    "def fs_score_fn(X_train, y_train, X_val, y_val, X_train_norm, X_val_norm, c, score_fn):\n",
    "    \n",
    "    num_sel_feat = np.arange(10, X_train.shape[1], 5)\n",
    "    \n",
    "    m,r,f = [], [], []\n",
    "    \n",
    "    for k in list(num_sel_feat):\n",
    "        print(\"\\n=======================Selected features %0.0f/%0.0f =======================\"%(k,X_train.shape[1]))\n",
    "        \n",
    "        # Applying feature selection algorithm supplied by the score_func on train, validation and normalized datasets\n",
    "        fs = SelectKBest(score_func=score_fn, k=k)         \n",
    "        fs.fit(X_train, y_train)\n",
    "                \n",
    "        # Retreive selected feature names\n",
    "        selected_features=X_train.columns[fs.get_support()]\n",
    "        \n",
    "        f.append(selected_features) \n",
    "        \n",
    "        # Get metrics based on selected features\n",
    "        models, results = models_comparison(X_train[selected_features], y_train, X_val[selected_features], y_val,\n",
    "                            c, False, False, X_train_norm[selected_features], X_val_norm[selected_features])\n",
    "        \n",
    "        m.append(models)\n",
    "        r.append(results)\n",
    "         \n",
    "    return m,r,num_sel_feat,f \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dac7fe43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Added new function else old one can be modified to accept start_interval_range and step_size.. Will update in next run\n",
    "# Function to plot regression metrics\n",
    "def plot_regression_metrics_reduced_features(X_train, num_sel_feat, results, fs_name):\n",
    "    \n",
    "    mse, r2 = [], []\n",
    "    for iteration in results:\n",
    "        mse.append(min([mse[2] for mse in iteration]))\n",
    "        r2.append(max([r2[0] for r2 in iteration]))\n",
    "    \n",
    "    # DO LATER:: modify original plotting function to accept start_interval_range and step_size, 30,30 in this case\n",
    "    num_sel_feat = np.arange(30, X_train.shape[1], 30) \n",
    "\n",
    "    # Plot R2 and MSE \n",
    "    plt.figure(figsize=(10, 4), dpi = 95)\n",
    "    sns.set_theme(style=\"darkgrid\")\n",
    "\n",
    "    sns.lineplot(x = num_sel_feat, y = mse, marker='o', label = 'MSE')\n",
    "    sns.lineplot(x = num_sel_feat, y = r2, marker='o', label = 'R2')\n",
    "    \n",
    "    # Vertical line to indicate best performance\n",
    "    sns.lineplot([num_sel_feat[np.argmax(r2)], num_sel_feat[np.argmax(r2)]], [0, max(mse)], \n",
    "                 color = 'red',linewidth = 4, label = 'Best')\n",
    "    plt.legend()\n",
    "    plt.title('MSE and R2 scores vs number of best features selected')\n",
    "    plt.xlabel('Number of selected features by %s'%fs_name)\n",
    "    plt.ylabel('Metrics')\n",
    "    plt.show()\n",
    "    return mse, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "166529d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data_balance(df, y_train, y_val, y_test):\n",
    "    sns.set_theme(style=\"darkgrid\")\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
    "    fig.suptitle('Counts of 0s and 1s in each split')\n",
    "\n",
    "    sns.countplot(x=\"Y\",ax=axes[0, 0], data = df, label = 'Complete dataset')\n",
    "    axes[0, 0].set_title('Full dataset')\n",
    "    sns.barplot(ax=axes[0, 1], x= y_train.value_counts().index, y= y_train.value_counts(), label= 'Training')\n",
    "    axes[0, 1].set_title('Training')\n",
    "    sns.barplot(ax=axes[1, 0], x= y_val.value_counts().index, y= y_val.value_counts(), label = 'Validation')\n",
    "    axes[1, 0].set_title('Validation')\n",
    "    sns.barplot(ax=axes[1, 1], x= y_test.value_counts().index, y= y_test.value_counts(), label = 'Test')\n",
    "    axes[1, 1].set_title('Test')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cda23ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_classification_metrics(results, X_train, fs_name):\n",
    "    mcc, auc = [], []\n",
    "    for iteration in results:\n",
    "        mcc.append(max([mcc[0] for mcc in iteration]))\n",
    "        auc.append(max([auc[1] for auc in iteration]))\n",
    "\n",
    "    num_sel_feat = np.arange(10, X_train.shape[1], 5)\n",
    "\n",
    "    # Plot R2 and MSE \n",
    "    plt.figure(figsize=(10, 4), dpi = 95)\n",
    "    sns.set_theme(style=\"darkgrid\")\n",
    "\n",
    "    sns.lineplot(x = num_sel_feat, y = mcc, marker='o', label = 'MCC')\n",
    "    sns.lineplot(x = num_sel_feat, y = auc, marker='o', label = 'AUC')\n",
    "    \n",
    "    # Vertical line to indicate best performance\n",
    "    sns.lineplot([num_sel_feat[np.argmax(mcc)], num_sel_feat[np.argmax(mcc)]], [0, max(auc)], \n",
    "                 color = 'red',linewidth = 4, label = 'Best')\n",
    "    \n",
    "    # Legend, title, and labels\n",
    "    plt.legend(loc= 'best')\n",
    "    plt.title('MCC and AUC scores vs number of best features selected')\n",
    "    plt.xlabel('Number of features selected by %s'%fs_name)\n",
    "    plt.ylabel('Metrics')\n",
    "    plt.show()\n",
    "    return mcc, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e6cb01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying various feature selection algorithm supplied by the score_func\n",
    "def fs_score_fn_selected_features(X_train, y_train, X_val, y_val, X_train_norm, X_val_norm, c, score_fn):\n",
    "    \n",
    "    #num_sel_feat = np.arange(10, X_train.shape[1], 5)\n",
    "    num_sel_feat = np.arange(30, X_train.shape[1], 30)\n",
    "    \n",
    "    m,r,f = [], [], []\n",
    "    \n",
    "    for k in list(num_sel_feat):\n",
    "        print(\"\\n=======================Selected features %0.0f/%0.0f =======================\"%(k,X_train.shape[1]))\n",
    "        \n",
    "        # Applying feature selection algorithm supplied by the score_func on train, validation and normalized datasets\n",
    "        fs = SelectKBest(score_func=score_fn, k=k)         \n",
    "        fs.fit(X_train, y_train)\n",
    "                \n",
    "        # Retreive selected feature names\n",
    "        selected_features=X_train.columns[fs.get_support()]\n",
    "        \n",
    "        f.append(selected_features) \n",
    "        \n",
    "        # Get metrics based on selected features\n",
    "        models, results = models_comparison(X_train[selected_features], y_train, X_val[selected_features], y_val,\n",
    "                            c, False, False, X_train_norm[selected_features], X_val_norm[selected_features])\n",
    "        \n",
    "        m.append(models)\n",
    "        r.append(results)\n",
    "         \n",
    "    return m,r,num_sel_feat,f "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6a834ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_classification_metrics_reduced_features(X_train, num_sel_feat, results, fs_name):\n",
    "    mcc, auc = [], []\n",
    "    for iteration in results:\n",
    "        mcc.append(max([mcc[0] for mcc in iteration]))\n",
    "        auc.append(max([auc[1] for auc in iteration]))\n",
    "\n",
    "    num_sel_feat = np.arange(30, X_train.shape[1], 30)\n",
    "\n",
    "    # Plot R2 and MSE \n",
    "    plt.figure(figsize=(10, 4), dpi = 95)\n",
    "    sns.set_theme(style=\"darkgrid\")\n",
    "\n",
    "    sns.lineplot(x = num_sel_feat, y = mcc, marker='o', label = 'MCC')\n",
    "    sns.lineplot(x = num_sel_feat, y = auc, marker='o', label = 'AUC')\n",
    "    \n",
    "    # Vertical line to indicate best performance\n",
    "    sns.lineplot([num_sel_feat[np.argmax(mcc)], num_sel_feat[np.argmax(mcc)]], [0, max(auc)], \n",
    "                 color = 'red',linewidth = 4, label = 'Best')\n",
    "    \n",
    "    # Legend, title, and labels\n",
    "    plt.legend(loc= 'best')\n",
    "    plt.title('MCC and AUC scores vs number of best features selected')\n",
    "    plt.xlabel('Number of features selected by %s'%fs_name)\n",
    "    plt.ylabel('Metrics')\n",
    "    plt.show()\n",
    "    return mcc, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e27813",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
